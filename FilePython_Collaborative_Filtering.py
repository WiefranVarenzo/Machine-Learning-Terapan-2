# -*- coding: utf-8 -*-
"""Collaborative_Filtering_final4_Last (2).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1EH1knOVXh1oci5l0jdyOBK3gaLZFiUdZ

# **Recommendation System: Retail Global Fashion Retail Sales**

*   Nama    : Wiefran Varenzo
*   Email   : lionwiefran88@gmail.com
*   Username: Wiefran Varenzo

Proyek ini bertujuan untuk: Melakukan rekomendasi menggunakan metode Collaborative Filtering yang bertujuan untuk mengefisiensikan penjualan produk fashion untuk retail ataupun e-commerce

# **1. Memuat Library dan Dataset**

## **Gambaran Umum Dataset****

Dataset ini diambil dari [Kaggle Repository](https://www.kaggle.com/datasets/ricgomes/global-fashion-retail-stores-dataset?select=transactions.csv) dengan judul:
**"Global Fashion Retail Sales"**

---



Berikut pembagian atribut ke dalam 4 kelompok dataset yang lebih terpisah dan terstruktur, lengkap dengan **tipe data** tiap kolom:

---

### **Transaksi (Sales Transaction Dataset)**

Berisi data transaksi individual pelanggan.

| Kolom            | Deskripsi                                           | Tipe Data       |
| ---------------- | --------------------------------------------------- | --------------- |
| Invoice ID       | ID unik untuk transaksi                             | `string`        |
| Line             | Nomor urut item dalam invoice                       | `int`           |
| Customer ID      | ID unik pelanggan                                   | `string`        |
| Product ID       | ID unik produk                                      | `string`        |
| Size             | Ukuran produk (S, M, L, XL, atau kosong)            | `string / null` |
| Color            | Warna produk (versi transaksi)                      | `string`        |
| Unit Price       | Harga satuan produk sebelum diskon                  | `float`         |
| Quantity         | Jumlah unit dibeli                                  | `int`           |
| Date             | Tanggal dan waktu transaksi                         | `datetime`      |
| Discount         | Diskon (misalnya 0.2 berarti 20%)                   | `float`         |
| Line Total       | Total harga line item setelah diskon                | `float`         |
| Store ID         | ID toko tempat transaksi                            | `string`        |
| Employee ID      | ID karyawan yang memproses transaksi                | `string`        |
| Currency         | Kode mata uang (3 huruf)                            | `string`        |
| Currency Symbol  | Simbol mata uang                                    | `string`        |
| SKU              | Gabungan Product ID, Size, dan Color                | `string`        |
| Transaction Type | Jenis transaksi (Sale, Return)                      | `category`      |
| Payment Method   | Metode pembayaran                                   | `category`      |
| Invoice Total    | Total invoice (sama untuk setiap baris per invoice) | `float`         |

---

### **Pelanggan (Customer Dataset)**

Berisi informasi demografis pelanggan.

| Kolom         | Deskripsi               | Tipe Data  |
| ------------- | ----------------------- | ---------- |
| Customer ID   | ID unik pelanggan       | `string`   |
| Name          | Nama pelanggan          | `string`   |
| Email         | Email pelanggan         | `string`   |
| Telephone     | Nomor telepon pelanggan | `string`   |
| City          | Kota pelanggan          | `string`   |
| Country       | Negara pelanggan        | `string`   |
| Gender        | Jenis kelamin (F, M, D) | `category` |
| Date Of Birth | Tanggal lahir pelanggan | `date`     |
| Job Title     | Pekerjaan pelanggan     | `string`   |
| Age           | Umur pelanggan          | `int`      |

---

### **Produk (Product Dataset)**

Berisi informasi produk dan deskripsinya.

| Kolom           | Deskripsi                   | Tipe Data      |
| --------------- | --------------------------- | -------------- |
| Product ID      | ID unik produk              | `string`       |
| Category        | Kategori produk utama       | `string`       |
| Sub Category    | Subkategori produk          | `string`       |
| Description PT  | Deskripsi produk (Portugis) | `string`       |
| Description DE  | Deskripsi produk (Jerman)   | `string`       |
| Description FR  | Deskripsi produk (Perancis) | `string`       |
| Description ES  | Deskripsi produk (Spanyol)  | `string`       |
| Description EN  | Deskripsi produk (Inggris)  | `string`       |
| Description ZH  | Deskripsi produk (Mandarin) | `string`       |
| Color           | Warna produk (versi produk) | `string`       |
| Sizes           | Ukuran produk yang tersedia | `list<string>` |
| Production Cost | Biaya produksi dalam USD    | `float`        |

---

### **Toko (Store Dataset)**

Berisi informasi geografis dan operasional tiap toko.

| Kolom               | Deskripsi                        | Tipe Data |
| ------------------- | -------------------------------- | --------- |
| Store ID            | ID toko                          | `string`  |
| Store Name          | Nama toko                        | `string`  |
| Number of Employees | Jumlah karyawan di toko          | `int`     |
| ZIP Code            | Kode pos toko                    | `string`  |
| City                | Kota toko                        | `string`  |
| Country             | Negara toko                      | `string`  |
| Latitude            | Koordinat lintang toko           | `float`   |
| Longitude           | Koordinat bujur toko             | `float`   |
| Country\_English    | Nama negara dalam bahasa Inggris | `string`  |

## **Import Libary**

Disini, saya melakukan mengimport pustaka yang dipakai, disini karena saya menggunakan TF-IDF maka library yang dipakai adalah TfidfVectorizer, kemudian, disini juga menggunakan cosine_similarity serta NearestNeighbors. Semua library ini di import di awal agar kode lebih mudah dilihat (library mana yang dipakai)
"""

import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from datetime import datetime

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler

from google.colab import files

"""## **Memuat Dataset**

Disini kita harus mengupload file Kaggle.json agar memiliki izin untuk mendownload dari website kaggle
"""

files.upload()

"""Disini kita akan menyambungkan kaggle dengan colab"""

!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json
!ls ~/.kaggle

"""Disini, kita ingin mendownload dataset kaggle dengan Kaggle download from CLI"""

!kaggle datasets download ricgomes/global-fashion-retail-stores-dataset

"""Disini, saya membuat direktori baru dengan nama global-fashion-retail-stores-dataset, setelah itu hasil download yang berupa zip akan di unsip dan dimasukkan ke direktori global-fashion-retail-stores-dataset"""

!mkdir global-fashion-retail-stores-dataset
!unzip global-fashion-retail-stores-dataset.zip -d global-fashion-retail-stores-dataset

"""Setiap dataset csv akan di muat ke variabel dengan pustaka dari pandas, yaitu fungsi read_csv()"""

customers = pd.read_csv('/content/global-fashion-retail-stores-dataset/customers.csv')
products = pd.read_csv('/content/global-fashion-retail-stores-dataset/products.csv')
stores = pd.read_csv('/content/global-fashion-retail-stores-dataset/stores.csv')
transactions = pd.read_csv('/content/global-fashion-retail-stores-dataset/transactions.csv')

"""Saya hanya menggunakan 4 dataset dari 6 dataset, dikarenakan untuk dataset Employees dan Discount tidak terlalu dibutuhkan, sebab yang yang lebih difokuskan disini adalah customer, produk, toko, serta transaksi yang terjadi (dalam hal recommendation system)

# **2. Data Understanding and EDA (Exploratory Data Analysis)**

## **Customer**
"""

customers.head()

"""Disini bisa kita lihat, bahwa ada beberapa data NaN, namun untuk bentuk datanya sendiri kebanyakan bertipe string/object yang memiliki informasi mengenai customer dari toko retail fashion"""

print('Jumlah data pelanggan: ', len(customers['Customer ID'].unique()))

"""Jumlah pelanggan yang kita miliki informasi berkitar 1,64 juta orang customer."""

customers.info()

"""Dari informasi yang bisa kita dapatkan, bahwa ada ketidak samaan untuk jumlah data Job Title dibandingkan kolom lainnya. Kemudian, untuk tipe data dari Date Of Birth seharusnya adalah bertipe 'DateTime'"""

customers.isnull().sum()

"""Ternyata memang terdapat data null di kolom Job Title, sebanyak 584185 data null yang nanti harus kita bersihkan"""

customers.duplicated().sum()

"""Tidak ditemukan adanya data yang duplikat di dataset customers."""

# Ubah kolom Date Of Birth menjadi format datetime (jika belum)
customers['Date Of Birth'] = pd.to_datetime(customers['Date Of Birth'], errors='coerce')

# Hitung usia (Age)
today = pd.to_datetime('today')
customers['Age'] = customers['Date Of Birth'].apply(lambda dob: (today.year - dob.year - ((today.month, today.day) < (dob.month, dob.day))) if pd.notnull(dob) else None)

"""Disini, saya mengubah aja langsung untuk tipe data Date Of Birth agar nantinya mempermudah proses preprocessing"""

# Distribusi Gender
sns.countplot(data=customers, x='Gender')
plt.title("Distribusi Gender Pelanggan")
plt.show()

"""Dataset ini memiliki data customer bergender M-Male lebih banyak dibandingkan F-Female. Sedangkan Diverse atau selain Wanita atau Pria, itu sangat sedikit."""

# Distribusi Umur
sns.histplot(customers['Age'].dropna(), bins=20, kde=True)
plt.title("Distribusi Umur Pelanggan")
plt.xlabel("Umur")
plt.show()

"""Distribusi cukup dominan dari kalangan orang mudah yang didominasi umur 20 tahunan

## **Products**
"""

products.head()

"""Ini adalah Dataset Product, yang merupakan dataset untuk informasi berkaitan dengan produk yang dijual retail fashion di seluruh dunia."""

print('Jumlah data pelanggan: ', len(products['Sub Category'].unique()))
print(products['Sub Category'].unique())

"""Ini adalah beberapa sub kategori dari produk yang di jual di retail fashion, terdapat 21 sub kategori yang berhubungan dengan pakaian dan juga barang fashion lainnya."""

products.info()

"""Disini, bisa kita lilhat terdapat beberapa perbedaan jumlah untuk color dan sizes, tetapi hal ini bisa berindikasi pada pemakaian berulang untuk setiap data color dan sizes yang tetap aman berada di dataset"""

# Distribusi Kategori
sns.countplot(data=products, x='Category', order=products['Category'].value_counts().index)
plt.title("Distribusi Kategori Produk")
plt.xticks(rotation=45)
plt.show()

"""Kategori dataset ini sendiri ada 3 macam yang di dominasi oleh produk berkategori feminime."""

# Distribusi Sub-Kategori
sns.countplot(data=products, x='Sub Category', order=products['Sub Category'].value_counts().index)
plt.title("Distribusi Sub-Kategori Produk")
plt.xticks(rotation=90)
plt.show()

"""Sub Kategori produk yang paling banyak ditemukan adalah Accesories, lalu diikut oleh Pants and jeans, Sportwear, dan coats and Blazers

## **Stores**
"""

stores.head()

"""Untuk dataset Stores, lebih mengarah kepada data toko retail seperti dimana negara dan kota toko itu berada, jumlah karyawan, ataupun zip code daerah."""

print('Jumlah data pelanggan: ', len(stores['Store Name'].unique()))
print(stores['Store Name'].unique())

"""Sekitar 35 toko yang telah didata"""

stores.info()

"""Dari informasi yang diberikan, bisa dilihat bahwa setiap tipe data sudah baik, dan sesuai."""

stores.Country.unique()

# Mapping nama negara agar semuanya pakai Latin characters
country_map = {
    'United States': 'United States',
    '中国': 'China',
    'Deutschland': 'Germany',
    'United Kingdom': 'United Kingdom',
    'France': 'France',
    'España': 'Spain',
    'Portugal': 'Portugal'
}

# Buat kolom baru hasil mapping
stores['Country_English'] = stores['Country'].map(country_map)

# Plot menggunakan kolom Country_English
sns.countplot(data=stores, x='Country_English', order=stores['Country_English'].value_counts().index)
plt.title("Jumlah Toko per Negara")
plt.xticks(rotation=45)
plt.show()

"""Penetapan langsung nama negara dari setiap toko, itu bertujuan agar mengurangi ketidak jelasan pada nama, terutama untuk nama china yang tidak akan dikenali oleh Colab, sehingga secara manual melakukan penamaan negara bisa mengurangi eror yang terjadi pada penamaan.

## **Transactions**
"""

transactions.head()

print('Jumlah data pelanggan: ', len(transactions['Invoice ID'].unique()))

"""Penamaan dataset transaksi berkisar 4,5 juta transaksi yang bisa di telaah dengan baik. Perhitungan ini dilakukan dengan menghitung Invoice ID agar bisa secara unik menghitung transaksi yang telah dilakukan"""

transactions.info()

"""Untuk data transaksi terdiri dari 19 kolom yang semuanya berkaitan dengan transaksi yang dilakukan oleh customer."""

# Distribusi Metode Pembayaran
sns.countplot(data=transactions, x='Payment Method', order=transactions['Payment Method'].value_counts().index)
plt.title("Distribusi Metode Pembayaran")
plt.show()

"""Untuk distribusi metode pembayaran, credit card disini sangat mendominasi metode pembayaran yang dilakukan. Sedangkan untuk metode pembayaran cash sendiri, tidak sampai 30% dari total pembayaran"""

# Distribusi Tipe Transaksi
sns.countplot(data=transactions, x='Transaction Type')
plt.title("Distribusi Sales and Return")
plt.show()

"""Lalu, dari grafik ini, bisa kita lihat bahwa barang yang dikembalikan sangat sedikit."""

# Ubah kolom 'Date' menjadi datetime
transactions['Date'] = pd.to_datetime(transactions['Date'])

# Penjualan Harian
daily_sales = transactions.groupby(transactions['Date'].dt.date)['Line Total'].sum().reset_index()

# Visualisasi
plt.figure(figsize=(12, 6))
sns.lineplot(data=daily_sales, x='Date', y='Line Total')
plt.title("Total Penjualan per Hari")
plt.xlabel("Tanggal")
plt.ylabel("Total Penjualan")
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

"""Kita bisa melihat bahwa terjadi beberapa lonjakan penjualan, terutama untuk yang berdekatan dengan januari (sebelum januari tahun depan) yang mengindikasikan berdekatan dengan natal, penjualan fashion meningkat cukup signifikan"""

# Korelasi Numerik
plt.figure(figsize=(10,6))
corr = transactions[['Unit Price', 'Quantity', 'Discount', 'Line Total', 'Invoice Total']].corr()
sns.heatmap(corr, annot=True, cmap='coolwarm', fmt='.2f')
plt.title("Korelasi Fitur Numerik Transaksi")
plt.show()

"""Kita dapat melihat bahwa Unit Price dan Line Total memiliki korelasi positif yang sangat kuat, begitu juga antara Invoice Total dan Line Total. Selain itu, terdapat korelasi yang cukup tinggi antara Invoice Total dan Unit Price.Hal ini mengindikasikan bahwa harga satuan (unit price) memainkan peran penting dalam menentukan nilai total transaksi, baik pada tingkat item (line total) maupun keseluruhan invoice (invoice total). Dengan kata lain, semakin tinggi harga satuan suatu produk, semakin besar kemungkinan total nilai transaksi juga meningkat, terutama jika jumlah pembelian (quantity) dan diskon tetap atau tidak bervariasi secara ekstrem."""

# Gabungkan transactions dengan products untuk mendapatkan deskripsi produk
transactions_with_desc = transactions.merge(products[['Product ID', 'Sub Category']], on='Product ID', how='left')

# Produk Terlaris berdasarkan Quantity
top_products = (
    transactions_with_desc.groupby('Sub Category')['Quantity']
    .sum()
    .sort_values(ascending=False)
    .head(10)
)

plt.figure(figsize=(12, 6))
top_products.plot(kind='bar', title="Top 10 Produk Terlaris", color='skyblue')
plt.xlabel("Deskripsi Produk")
plt.ylabel("Jumlah Terjual")
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.show()

"""Dalam data transaksi ini, kita dapat melihat bahwa terdapat 10 produk terlaris, di antaranya adalah Pants and Jeans, Sportswear, dan lainnya.
Beberapa kategori seperti Dresses and Jumpsuits, T-shirts and Polos, Shirts and Blouses, Sweaters-Sweatshirts, serta Sweaters and Knitwear menunjukkan jumlah penjualan yang hampir setara, yang mengindikasikan bahwa kategori-kategori tersebut memiliki tingkat permintaan yang relatif merata di antara konsumen.


"""

# Produk dengan Pendapatan Tertinggi berdasarkan Line Total
top_revenue_products = (
    transactions_with_desc.groupby('Sub Category')['Line Total']
    .sum()
    .sort_values(ascending=False)
    .head(10)
)

plt.figure(figsize=(12, 6))
top_revenue_products.plot(kind='bar', title="Top 10 Produk Berpendapatan Tertinggi", color='seagreen')
plt.xlabel("Deskripsi Produk")
plt.ylabel("Total Pendapatan")
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.show()

"""Coats and Blazers dan pants-jeans adalah produk yang mendatangkan pendapatan paling tinggi, hal ini bisa terjadi karena memang kedua produk ini terjual paling banyak dari produk lainnya. Namun yang menariknya suits and blazers tidak ada pada top 10 paling banyak terjual, tetapi menjadi top 3 barang yang membawa pendapatan tertinggi. Hal ini bisa dikarenakan harga barang saat terjual jauh lebih tinggi dari produk lainnya.

# **3. Data Preprocessing dan Preparation**

## **Mengatasi Data Null dan Duplikat**
"""

customers = customers.dropna(axis=0, how='any')        # Hapus baris dengan nilai null
customers = customers.drop_duplicates()                # Hapus duplikat seluruh baris

"""Disini, saya menghapus baris null dan duplikat untuk dataset customer"""

products = products.dropna(axis=0, how='any')          # Hapus baris dengan nilai null
products = products.drop_duplicates()

"""Disini, saya menghapus baris null dan duplikat untuk dataset products"""

stores = stores.dropna(axis=0, how='any')
stores = stores.drop_duplicates()

"""Disini, saya menghapus baris null dan duplikat untuk dataset stores"""

transactions = transactions.dropna(axis=0, how='any')
transactions = transactions.drop_duplicates()

"""Disini, saya menghapus baris null dan duplikat untuk dataset transactions"""

transactions = transactions.dropna(subset=['Customer ID', 'Product ID', 'Store ID', 'Quantity'])

"""Disini, saya kembali memastikan bahwa untuk kolom-kolom penting yang nanti akan dipakai tidak memiliki data null

## **Menggabungkan seluruh data berdasarkan ID masing-masing dataset**
"""

# Merge hanya yang match (tanpa NULL)
df = transactions.merge(customers, on='Customer ID', how='inner') \
                 .merge(products, on='Product ID', how='inner') \
                 .merge(stores, on='Store ID', how='inner')

"""Disini, saya menggabungkan dataset transactions dengan dataset customers, products, dan stores, yang di merge dengan metode inner join, sehingga yang menggabungkan setiap ID dengan dataset transactions"""

print(df.info())         # Lihat struktur & null
print(df.isnull().sum()) # Cek sisa null per kolom

"""Dari sini kita bisa melihat bahwa tidak ada data null sama sekali untuk dataset gabungan.

## **Mengambil data yang hanya diperlukan untuk Collaborative Filtering**
"""

# Pilih kolom-kolom penting untuk CF dan CBF
df = df[['Customer ID', 'Name', 'Gender', 'Age', 'Product ID', 'Store Name',
         'Quantity', 'Line Total', 'Date', 'Category', 'Sub Category',
         'Color_y', 'Sizes', 'Description EN', 'Production Cost']]

df

# Data untuk CF
cf_data = df[['Customer ID', 'Product ID', 'Line Total']].copy()
# Drop baris yang Line Total-nya kosong atau negatif
cf_data = cf_data[cf_data['Line Total'] > 0].copy()

# Hitung rating
cf_data['rating'] = np.log1p(cf_data['Line Total'])
scaler = MinMaxScaler()
cf_data['rating'] = scaler.fit_transform(cf_data[['rating']])

"""Pada tahap ini, data disiapkan untuk keperluan Collaborative Filtering (CF) dengan cara memilih kolom yang relevan, yaitu Customer ID, Product ID, dan Line Total. Baris dengan nilai Line Total yang kosong atau bernilai negatif dihapus untuk menjaga validitas data transaksi. Kemudian, nilai Line Total digunakan untuk menghitung rating sebagai representasi minat pelanggan terhadap produk, dengan menerapkan transformasi logaritmik log1p guna mereduksi skala nilai dan mengatasi skewness. Setelah itu, rating yang dihasilkan dinormalisasi ke dalam rentang [0, 1] menggunakan MinMaxScaler dari sklearn, yang bertujuan untuk menyeimbangkan distribusi data dan membantu proses pelatihan model menjadi lebih stabil."""

user_ids = cf_data['Customer ID'].unique().tolist()
product_ids = cf_data['Product ID'].unique().tolist()

user2user_encoded = {x: i for i, x in enumerate(user_ids)}
product2product_encoded = {x: i for i, x in enumerate(product_ids)}

cf_data['user'] = cf_data['Customer ID'].map(user2user_encoded)
cf_data['product'] = cf_data['Product ID'].map(product2product_encoded)

num_users = len(user2user_encoded)
num_products = len(product2product_encoded)

cf_data = cf_data[['user', 'product', 'rating']]

"""Langkah ini bertujuan untuk mengubah ID pengguna (Customer ID) dan ID produk (Product ID) menjadi representasi numerik yang dapat diproses oleh model machine learning. Pertama, daftar unik ID pengguna dan produk diambil, lalu dibuat pemetaan dari ID asli ke indeks integer menggunakan enumerate(). Selanjutnya, kolom Customer ID dan Product ID dalam data di-map menjadi kolom user dan product menggunakan kamus yang telah dibuat. Proses ini menghasilkan dua kolom numerik yang mewakili pengguna dan produk. Jumlah total pengguna (num_users) dan produk (num_products) juga dihitung dan disimpan. Terakhir, dataset dibatasi hanya pada kolom user, product, dan rating untuk digunakan dalam proses pelatihan model.

# **4. Pembangunan Model Collaborative Filtering (RecommenderNet)**
"""

X = cf_data[['user', 'product']].values
y = cf_data['rating'].values

X_train, X_val, y_train, y_val = train_test_split(
    X, y, test_size=0.2, random_state=42
)

"""Disini saya melakukan pemisahan data training dan testing dimana test_size yang saya ambil adalah 0.2 atau 20% dari total dataset, serta data trainingnya adalah 80%"""

class RecommenderNet(tf.keras.Model):
    def __init__(self, num_users, num_items, embedding_size=50, **kwargs):
        super(RecommenderNet, self).__init__(**kwargs)
        self.user_embedding = layers.Embedding(
            input_dim=num_users, output_dim=embedding_size,
            embeddings_initializer='he_normal',
            embeddings_regularizer=tf.keras.regularizers.l2(1e-6)
        )
        self.item_embedding = layers.Embedding(
            input_dim=num_items, output_dim=embedding_size,
            embeddings_initializer='he_normal',
            embeddings_regularizer=tf.keras.regularizers.l2(1e-6)
        )
        self.dense_1 = layers.Dense(64, activation='relu')
        self.dropout = layers.Dropout(0.2)
        self.out = layers.Dense(1, activation='linear')  # regresi rating

    def call(self, inputs):
        user_vector = self.user_embedding(inputs[:, 0])
        item_vector = self.item_embedding(inputs[:, 1])
        x = user_vector * item_vector
        x = self.dense_1(x)
        x = self.dropout(x)
        return self.out(x)

"""Kode ini mendefinisikan arsitektur model rekomendasi berbasis deep learning bernama RecommenderNet menggunakan TensorFlow Keras. Model ini memanfaatkan pendekatan collaborative filtering dengan teknik embedding untuk merepresentasikan pengguna dan item (produk) dalam bentuk vektor berdimensi tetap. Dua layer embedding digunakan: satu untuk pengguna dan satu untuk item, masing-masing dengan ukuran dimensi yang ditentukan oleh embedding_size. Vektor embedding dari pengguna dan item kemudian dikalikan secara elemen (dot-wise product) untuk menangkap interaksi di antara keduanya. Hasil perkalian ini diteruskan ke dense layer dengan aktivasi ReLU, diikuti oleh dropout untuk mencegah overfitting, dan akhirnya dilewatkan ke output layer dengan aktivasi linear untuk memprediksi skor rating. Model ini dirancang untuk mempelajari pola interaksi pengguna dan item dari data historis."""

print(np.isnan(X_train).sum(), np.isinf(X_train).sum())
print(np.isnan(y_train).sum(), np.isinf(y_train).sum())

early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)
reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, min_lr=1e-6)

model = RecommenderNet(num_users, num_products, embedding_size=50)

model.compile(
    loss='mse',
    optimizer = keras.optimizers.Adam(learning_rate=0.0005),
    metrics=[tf.keras.metrics.RootMeanSquaredError()]
)
history = model.fit(
    x=X_train,
    y=y_train,
    batch_size=256,
    epochs=10,
    validation_data=(X_val, y_val),
    callbacks=[early_stop, reduce_lr]
)

"""## **Get Recommendation**"""

def recommend_products_with_customer_info(model, user_id_original, top_k=10):
    # Encode user ID
    user_id_encoded = user2user_encoded[user_id_original]

    # Info customer
    customer_info = df[df['Customer ID'] == user_id_original][['Customer ID', 'Name', 'Gender','Age']].drop_duplicates().reset_index(drop=True)

    # Produk yang belum dibeli
    products_bought = cf_data[cf_data['user'] == user_id_encoded]['product'].values
    all_product_indices = np.array([i for i in range(num_products) if i not in products_bought])

    # Prediksi skor
    user_array = np.full(len(all_product_indices), user_id_encoded)
    predictions = model.predict(np.stack([user_array, all_product_indices], axis=1)).flatten()

    # Ambil top-k
    top_indices = predictions.argsort()[-top_k:][::-1]
    recommended_product_ids = [product_ids[all_product_indices[i]] for i in top_indices]

    # Info produk
    recommended_df = df[df['Product ID'].isin(recommended_product_ids)][[
        'Product ID', 'Category', 'Sub Category', 'Color_y', 'Sizes', 'Description EN'
    ]].drop_duplicates().reset_index(drop=True)

    return customer_info, recommended_df

"""Fungsi recommend_products_with_customer_info dirancang untuk menghasilkan rekomendasi produk bagi pengguna tertentu dengan menyertakan informasi profil pengguna dan detail produk yang direkomendasikan. Pertama, fungsi ini mengubah ID pengguna asli menjadi ID numerik yang digunakan dalam model. Selanjutnya, informasi dasar pengguna seperti nama, gender, dan usia diambil dari dataset. Fungsi ini kemudian mengidentifikasi produk-produk yang belum dibeli oleh pengguna dan memprediksi skor preferensi pengguna terhadap produk-produk tersebut menggunakan model rekomendasi. Dari hasil prediksi, fungsi memilih top_k produk dengan skor tertinggi dan mengambil informasi deskriptif dari produk-produk tersebut seperti kategori, subkategori, warna, ukuran, dan deskripsi. Fungsi ini mengembalikan dua DataFrame: satu berisi informasi pengguna dan satu lagi berisi daftar produk yang direkomendasikan."""

user_id_sample = df['Customer ID'].iloc[0]  # atau pilih ID manual
customer_info, recommendation_result = recommend_products_with_customer_info(model, user_id_sample)

print("Informasi Pelanggan:\n")
print(customer_info.to_string(index=False))

print("\nRekomendasi Produk:\n")
print(recommendation_result.to_string(index=False))

"""## **Evaluation**"""

plt.figure(figsize=(10, 4))

plt.subplot(1, 2, 1)
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Loss over Epochs')
plt.xlabel('Epoch')
plt.ylabel('MSE Loss')
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(history.history['root_mean_squared_error'], label='Train RMSE')
plt.plot(history.history['val_root_mean_squared_error'], label='Val RMSE')
plt.title('RMSE')
plt.xlabel('Epoch')
plt.ylabel('RMSE')
plt.legend()

plt.tight_layout()
plt.show()

"""### **Evaluasi Model Regresi dengan Metrik MSE & RMSE (Data Terbaru)**

* **Performa Model Selama Training dan Validasi:**

  * Model menunjukkan **penurunan yang konsisten pada *training loss* dan RMSE** pada epoch awal. Misalnya, *loss* turun dari 0.0302 (RMSE 0.1704) pada Epoch 1 menjadi 0.0012 (RMSE 0.0256) pada Epoch 10.
  * Pada data validasi, **`val_loss` dan `val_RMSE` juga menunjukkan tren penurunan di awal**, dari 0.0217 (RMSE 0.1467) pada Epoch 1 ke nilai terendah sekitar 0.0124 (RMSE 0.1086) pada Epoch 9. Namun, setelah Epoch 6, terlihat **sedikit fluktuasi dan potensi kenaikan tipis pada `val_loss` dan `val_RMSE`** (misalnya, `val_loss` 0.0132 dan `val_RMSE` 0.1118 pada Epoch 8 setelah sebelumnya mencapai nilai lebih rendah), yang perlu diwaspadai sebagai tanda awal *overfitting* atau model mencapai batas kemampuannya dengan konfigurasi saat ini.

* **Konsistensi dan Stabilitas Pelatihan:**

  * *Gap* antara *training loss/RMSE* dan *validation loss/RMSE* mulai **sedikit melebar setelah Epoch 6**, di mana *training loss* terus menurun sementara *validation loss* cenderung stagnan atau sedikit naik. Ini mengindikasikan bahwa model mungkin mulai mempelajari *noise* pada data training.
  * Strategi penurunan *learning rate* (dari 5.0000e-04 ke 2.5000e-04 pada Epoch 6, lalu ke 1.2500e-04 pada Epoch 9) adalah langkah yang baik untuk *fine-tuning*. Penurunan *learning rate* ini tampaknya membantu menurunkan *training loss* lebih lanjut, tetapi belum signifikan memperbaiki *validation loss* di epoch-epoch terakhir yang Anda berikan.

* **Kualitas Prediksi Berdasarkan RMSE:**

  * RMSE training mencapai nilai yang sangat rendah (< 0.05), menunjukkan model sangat cocok dengan data training.
  * RMSE validasi stabil di kisaran 0.108 hingga 0.114 di epoch-epoch terakhir yang ditunjukkan. Kualitas prediksi pada data validasi ini cukup baik, namun perlu diperhatikan agar tidak memburuk jika pelatihan dilanjutkan tanpa penyesuaian.

* **Penurunan Metrik dan Potensi *Overfitting*:**

  * Penurunan tajam pada *training loss* dan *validation loss* terjadi hingga sekitar Epoch 3-6. Setelah itu, *training loss* masih menurun, tetapi *validation loss* tidak lagi menunjukkan penurunan signifikan, bahkan ada kecenderungan sedikit meningkat. Ini adalah **indikasi klasik bahwa model mungkin mendekati fase *overfitting*** atau sudah mencapai performa optimalnya pada data validasi dengan arsitektur dan *hyperparameter* saat ini.

---

### **Kesimpulan**

* Model regresi menunjukkan **pembelajaran yang sangat baik pada data training**.
* Performa pada data validasi **baik hingga Epoch 6**, namun setelah itu menunjukkan **tanda-tanda awal *overfitting* atau stagnasi**, di mana metrik validasi tidak membaik seiring metrik training yang terus membaik.
* Strategi penurunan *learning rate* sudah diterapkan, yang merupakan praktik yang baik.
* Dengan `val_RMSE` berada di sekitar 0.108 - 0.111, model ini masih bisa dianggap cukup *reliable*. Namun, sangat disarankan untuk **memantau dengan ketat beberapa epoch ke depan** atau mempertimbangkan strategi seperti **`EarlyStopping`** berdasarkan `val_loss` untuk mencegah *overfitting* yang lebih parah. Jika `val_loss` mulai meningkat secara konsisten, menghentikan pelatihan lebih awal adalah pilihan yang bijak.
* Pertimbangkan untuk menyimpan model pada epoch di mana `val_loss` mencapai nilai minimum (sekitar Epoch 6 atau 9 berdasarkan data ini, tergantung mana yang terendah jika dilihat lebih detail atau jika ada fluktuasi).

# **5. Pembangunan Model Collaborative Filtering (MatrixFactorization)**

Kelas MatrixFactorization merupakan implementasi model rekomendasi berbasis teknik faktorisasi matriks menggunakan TensorFlow. Model ini menggunakan dua layer embedding terpisah untuk merepresentasikan pengguna dan item (produk) dalam bentuk vektor berdimensi laten. Vektor-vektor embedding ini dipelajari selama proses pelatihan untuk merepresentasikan preferensi pengguna dan karakteristik item. Dalam metode call, model mengambil pasangan input pengguna dan produk, kemudian menghitung hasil perkalian dot product antara vektor embedding pengguna dan item untuk menghasilkan prediksi rating atau skor ketertarikan. Pendekatan ini merupakan salah satu teknik collaborative filtering yang sederhana namun efektif untuk merekomendasikan item berdasarkan interaksi historis antara pengguna dan produk.
"""

class MatrixFactorization(tf.keras.Model):
    def __init__(self, num_users, num_items, latent_dim=50):
        super(MatrixFactorization, self).__init__()
        self.user_embedding = tf.keras.layers.Embedding(
            input_dim=num_users, output_dim=latent_dim,
            embeddings_initializer='he_normal',
            embeddings_regularizer=tf.keras.regularizers.l2(1e-6)
        )
        self.item_embedding = tf.keras.layers.Embedding(
            input_dim=num_items, output_dim=latent_dim,
            embeddings_initializer='he_normal',
            embeddings_regularizer=tf.keras.regularizers.l2(1e-6)
        )

    def call(self, inputs):
        user_vector = self.user_embedding(inputs[:, 0])
        item_vector = self.item_embedding(inputs[:, 1])
        dot_product = tf.reduce_sum(user_vector * item_vector, axis=1, keepdims=True)
        return dot_product

mf_model = MatrixFactorization(num_users=num_users, num_items=num_products, latent_dim=50)

mf_model.compile(
    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
    loss='mse',
    metrics=[tf.keras.metrics.RootMeanSquaredError()]
)

history_mf = mf_model.fit(
    X_train, y_train,
    batch_size=256,
    epochs=10,
    validation_data=(X_val, y_val),
    callbacks=[early_stop, reduce_lr]
)

def recommend_products_mf(model, user_id_original, top_k=10):
    user_id_encoded = user2user_encoded[user_id_original]

    customer_info = df[df['Customer ID'] == user_id_original][['Customer ID', 'Name', 'Gender','Age']].drop_duplicates().reset_index(drop=True)

    products_bought = cf_data[cf_data['user'] == user_id_encoded]['product'].values
    all_product_indices = np.array([i for i in range(num_products) if i not in products_bought])

    user_array = np.full(len(all_product_indices), user_id_encoded)
    predictions = model.predict(np.stack([user_array, all_product_indices], axis=1)).flatten()

    top_indices = predictions.argsort()[-top_k:][::-1]
    recommended_product_ids = [product_ids[all_product_indices[i]] for i in top_indices]

    recommended_df = df[df['Product ID'].isin(recommended_product_ids)][[
        'Product ID', 'Category', 'Sub Category', 'Color_y', 'Sizes', 'Description EN'
    ]].drop_duplicates().reset_index(drop=True)

    return customer_info, recommended_df

"""Fungsi recommend_products_mf digunakan untuk menghasilkan rekomendasi produk berbasis model Matrix Factorization bagi pengguna tertentu. Fungsi ini dimulai dengan mengonversi ID pengguna asli menjadi ID numerik yang telah digunakan dalam pelatihan model. Informasi dasar pengguna seperti nama, jenis kelamin, dan usia diambil dari dataset utama. Selanjutnya, fungsi mengidentifikasi produk-produk yang belum pernah dibeli oleh pengguna tersebut dan memprediksi skor ketertarikan terhadap produk-produk tersebut menggunakan model Matrix Factorization yang telah dilatih sebelumnya. Berdasarkan hasil prediksi, fungsi memilih top_k produk dengan skor tertinggi sebagai rekomendasi. Produk-produk ini kemudian dilengkapi dengan informasi deskriptif seperti kategori, subkategori, warna, ukuran, dan deskripsi dalam bahasa Inggris. Hasil akhir dari fungsi ini adalah dua DataFrame: satu berisi informasi pengguna dan satu lagi berisi daftar produk yang direkomendasikan.

## **Get Recommendation**
"""

user_id_sample = df['Customer ID'].iloc[0]  # atau pilih ID manual
customer_info, recommendation_result = recommend_products_mf(model, user_id_sample)

print("Informasi Pelanggan:\n")
print(customer_info.to_string(index=False))

print("\nRekomendasi Produk:\n")
print(recommendation_result.to_string(index=False))

"""## **Evaluation**"""

def plot_evaluation(history):
    # Plot Loss
    plt.figure(figsize=(12, 5))

    # Plot Loss
    plt.subplot(1, 2, 1)
    plt.plot(history.history['loss'], label='Training Loss')
    plt.plot(history.history['val_loss'], label='Validation Loss')
    plt.title('Loss over Epochs')
    plt.xlabel('Epoch')
    plt.ylabel('MSE Loss')
    plt.legend()

    # Plot RMSE
    plt.subplot(1, 2, 2)
    plt.plot(history.history['root_mean_squared_error'], label='Training RMSE')
    plt.plot(history.history['val_root_mean_squared_error'], label='Validation RMSE')
    plt.title('RMSE over Epochs')
    plt.xlabel('Epoch')
    plt.ylabel('Root Mean Squared Error')
    plt.legend()

    plt.tight_layout()
    plt.show()

# Panggil fungsi untuk model MF
plot_evaluation(history_mf)

"""### **Evaluasi Model Regresi dengan Metrik MSE & RMSE (Data Terbaru)**

#### **Performa selama training dan validasi:**

* **Training Loss (MSE)** menunjukkan penurunan yang konsisten dan signifikan, dari 0.1759 pada Epoch 1 menjadi 0.0593 pada Epoch 10.
* **Validation Loss (MSE)** juga mengalami penurunan yang baik, dari 0.1254 pada Epoch 1 menjadi 0.0931 pada Epoch 10. Penurunan ini terlihat lebih landai setelah beberapa epoch awal, tetapi tetap menunjukkan tren positif.
* **Training RMSE** turun secara drastis dari 0.4165 ke 0.1658, dan **Validation RMSE** juga membaik dari 0.3251 ke 0.2474. Ini mengindikasikan bahwa model belajar dengan baik dan akurasinya meningkat pada kedua set data.

#### **Konsistensi dan Stabilitas Training:**

* Grafik (diasumsikan dari data) akan menunjukkan **penurunan yang stabil pada *training loss* dan RMSE**.
* *Gap* antara *training metrics* dan *validation metrics* ada, namun **tidak terlihat melebar secara signifikan yang mengindikasikan *overfitting*** parah dalam 10 epoch ini. Kurva validasi yang terus menurun, meskipun lebih lambat, adalah pertanda baik.
* Laju penurunan pada *validation loss/RMSE* melambat dibandingkan *training loss/RMSE*, yang merupakan hal wajar seiring model mendekati konvergensi atau batas kemampuannya dengan *learning rate* saat ini.

#### **Kualitas Prediksi dan Efisiensi:**

* RMSE validasi akhir sebesar 0.2474 mengindikasikan tingkat akurasi model. Jika nilai target data Anda berada pada skala kecil (misalnya, antara 0 dan 1), RMSE ini bisa dianggap cukup baik. Namun, jika skala target lebih besar, interpretasinya mungkin berbeda.
* Perbaikan pada *validation loss* dan *validation RMSE* menjadi semakin kecil di epoch-epoch terakhir (misalnya, dari Epoch 9 ke Epoch 10, `val_loss` turun dari 0.0936 ke 0.0931). Ini menandakan bahwa model mungkin **mendekati konvergensi atau *plateau*** dengan *learning rate* 0.0010 yang konstan.

#### **Visualisasi Kurva (Asumsi):**

* Kurva *training loss* dan *validation loss* (serta RMSE) akan menunjukkan tren penurunan yang sehat.
* Tidak ada tanda-tanda *validation loss* yang mulai naik secara konsisten, yang berarti **tidak ada *overfitting* yang jelas** dalam 10 epoch ini.

---

### **Kesimpulan Evaluasi**

* Model menunjukkan **progres pembelajaran yang baik dan stabil** pada 10 epoch pertama, dengan peningkatan akurasi yang terlihat pada data training maupun data validasi.
* Saat ini, model **tidak menunjukkan tanda-tanda *overfitting*** yang signifikan. *Validation loss* masih menurun, yang merupakan indikasi positif.
* Meskipun performa meningkat, laju perbaikan pada data validasi mulai melambat. Untuk optimasi lebih lanjut dan potensi peningkatan performa di epoch berikutnya, beberapa strategi bisa dipertimbangkan:

  * **Melanjutkan pelatihan** untuk beberapa epoch lagi dengan *learning rate* saat ini untuk melihat apakah *validation loss* bisa turun lebih jauh.
  * Jika *validation loss* mulai stagnan atau meningkat, terapkan **`EarlyStopping`**.
  * Menerapkan **pengurangan *learning rate*** (*learning rate decay* atau *scheduler*) setelah beberapa epoch stagnasi dapat membantu model melakukan *fine-tuning* dan mungkin mencapai konvergensi yang lebih baik.
* Secara keseluruhan, dengan `val_RMSE` sekitar 0.2474, model ini menunjukkan potensi yang baik dan dapat dilanjutkan ke tahap evaluasi lebih lanjut atau *fine-tuning* jika diperlukan.

### **6. Kesimpulan Umum Model**

Berdasarkan performa terkini dari kedua model regresi (RecommenderNet dan Matrix Factorization), berikut adalah kesimpulan yang telah diperbarui secara menyeluruh:

1. **RecommenderNet menunjukkan performa pelatihan sangat baik**, dengan *training loss* turun tajam dari 0.0302 menjadi 0.0012 dan *training RMSE* mencapai nilai rendah yaitu **0.0256** pada akhir epoch. Namun, setelah epoch ke-6, terlihat adanya **tanda-tanda awal overfitting**, di mana *validation loss* mulai sedikit meningkat dari 0.0124 ke 0.0132, dan *validation RMSE* naik tipis dari 0.1086 ke 0.1118 pada epoch ke-8 sebelum turun kembali sedikit di epoch ke-10 (**val\_RMSE akhir 0.1089**).

2. **Matrix Factorization menunjukkan pembelajaran stabil dan konsisten** selama 10 epoch, dengan *training loss* menurun dari 0.1755 ke 0.0582, dan *training RMSE* akhir sebesar **0.1658**. *Validation loss* dan *validation RMSE* juga menurun stabil dari 0.1254 ke 0.0931 dan dari 0.3251 ke **0.2474**, tanpa tanda-tanda overfitting.

3. **Perbandingan performa akhir** menunjukkan bahwa **RecommenderNet memiliki akurasi validasi yang lebih tinggi** (val\_RMSE 0.1089 vs 0.2474) dan belajar lebih baik pada data training, tetapi perlu diwaspadai potensi *overfitting* jika pelatihan dilanjutkan tanpa strategi seperti `EarlyStopping`.

4. **Matrix Factorization unggul dalam kestabilan**, dengan penurunan metrik yang konsisten tanpa fluktuasi berarti, cocok jika tujuan Anda adalah model yang lebih aman untuk generalisasi awal dan baseline.

5. **Rekomendasi:**

   * Jika Anda membutuhkan model dengan **akurasi tinggi**, pilih **RecommenderNet** dan **aktifkan `EarlyStopping`** untuk menghentikan pelatihan saat val\_loss mulai meningkat.
   * Jika Anda mengutamakan **stabilitas dan kemudahan kontrol**, Matrix Factorization tetap layak digunakan, meskipun performanya lebih rendah secara metrik.
"""